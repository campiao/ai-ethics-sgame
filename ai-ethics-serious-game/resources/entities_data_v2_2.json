{
	"0":{
		"question_text": "CASE #0420 — After deploying a diagnostic AI trained on millions of medical records, a hospital proposed removing doctors from certain review steps. How much should we trust the AI?",
		"answer_text": "The AI shows high accuracy in tests but lacks contextual judgment in rare or ethical cases.",
		"confidence_target": 30,
		"feedback_perceptive": "Human doctors stayed involved. Misdiagnoses decreased, and patient trust remained high.",
		"feedback_missed": "Over- or under-reliance caused delays or errors in diagnoses, shaking public confidence in the hospital."
  	},
  	"1":{
		"question_text": "CASE #0875 — A chatbot gave confident legal advice that seemed credible. A user acted on it and suffered financial loss. Should people rely on confident AI outputs?",
		"answer_text": "The chatbot generated its response based on patterns in public forums, not verified sources.",
		"confidence_target": 20,
		"feedback_perceptive": "Users learned to double-check. Misinformation cases dropped, and usage remained stable.",
		"feedback_missed": "Blind trust or total doubt led to harm,  either spreading false info or ignoring valid help."
  	},
  	"2":{
		"question_text": "CASE #2311 — An AI-created artwork won an international prize. Later, several artists claimed parts of their work were used. How original is this image?",
		"answer_text": "The image was generated by recombining visual elements from a large dataset of prior works.",
		"confidence_target": 70,
		"feedback_perceptive": "Organizers clarified rules. Artists and audiences accepted the decision as fair.",
		"feedback_missed": "The contest faced criticism for unclear boundaries, losing trust from the art community."
  	},
  	"3":{
		"question_text": "CASE #3194 — A hiring AI system rejected many candidates from a specific background. The company defended its neutrality. Can this system be trusted to be fair?",
		"answer_text": "The AI was trained on past decisions that reflected biased hiring practices.",
		"confidence_target": 25,
		"feedback_perceptive": "The system was audited. Hiring became more transparent, with improved fairness metrics.",
		"feedback_missed": "Neglecting bias concerns led to public backlash and internal resignations."
  	}
}
